## 500px Machine Learning Engineer Intern - Tech Challenge

This is my solution to "500px Machine Learning Engineer Intern - Tech Challenge".

#### Environment

* Python 3.6
* Tensorflow 1.0
* Numpy 1.11.3
* Matplotlib 2.0.0
* Seaborn 0.71

#### Description

Create adversarial images to fool a MNIST classifier in TensorFlow.

There are two ways to fool a CNN model. The first one is raised by Ian Goodfellow (also known
as the fast gradient sign method), the other one is adopted by Papernot et al (also know as Jacobian-based saliency map). Here we first implement the easier one: fast gradient method.

This method can be simplified as

<img src="/img/fg_euqtion.png" width="250">

We tested two update strategies, one is to simply take the sign of gradient and perform uniformly updates for all pixels and the other is to use gradient value as parts of step size. It is easy to know that if the gradient of a pixel is large, we only need to add a little wiggle, which will bring enough influence on the prediction. Therefore, it will give us a less noisy adversarial image. We add a L2-norm -like loss to evaluate the Delta, a small loss means we add a very little perturbation to original image, making the adversarial image unnoticeable for humans. 

The experiment results support that -- updating by gradient value gives us a less L2-norm-like loss.

Here is the result for using the sign of gradient, the final L2-norm-like loss is 3.12143

![](/img/fg_sign_versus.png)

![](/img/fg_sign_iter.png)

Here is the result for using the value of gradient, the final L2-norm-like loss is 2.18394

![](/img/fg_grad_versus.png)

![](/img/fg_grad_iter.png)

As we want an adversarial image with unnoticeable perturbation, we perform gradient value updating.

You can find experiments result in the `notebook` folder

In `src` folder, `generate_model.py` is used for training CNN model and dump model to target path

Usage:

```
python generate_model.py [--max_steps] [--learning_rate] [--dropout] [--data_dir]
                         [--log_dir] [--model_path]
```

`generate_adversarial.py` contains function that consumes a image list(img_list) and a desired class(target_class) , therefore produces the adversarial images that can fool the model.

This following image is the result generated by 

<img src="/img/adversarial_versus.png" width="500">

You can find iteration processes in the `img` folder

#### To Do

* [x] Read [Karpathy's blog](http://karpathy.github.io/2015/03/30/breaking-convnets/)
* [x] Read paper [Deep Neural Networks are Easily Fooled](https://arxiv.org/abs/1412.1897) and [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)
* [x] Go through all details in [Deep MNIST for Experts](https://www.tensorflow.org/get_started/mnist/pros)
* [x] Here is a [torch implement](https://github.com/e-lab/torch-toolbox/tree/master/Adversarial) for OVERFEAR network (Not so useful)
* [ ] Time to code ~
      * [x] Train a common CNN for MNIST
      * [x] generate adversarial images to misclassify any examples of ‘2’ as ‘6’
      * [x] generate adversarial images for any number as any number
      * [ ] *test SVM, RandomForest, GradientBoost Tree, deeper network for these adversarial images
      * [ ] what if we add adversarial images in the training set?
      * [x] *test my handwritten image
      * [ ] *analyze feature map for conv layer
      * [ ] *implement Jacobian-based saliency map described in [This paper](https://arxiv.org/abs/1511.07528)